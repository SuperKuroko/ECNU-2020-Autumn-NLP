{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作业五 相似度计算  \n",
    "任务描述：采用word2vec方法，进行句子相似度计算训练。  \n",
    "  \n",
    "给出一个有关句子相似度的二分类数据集msr_paraphrase（包含train、test、README三个文件），其中第一列数字1代表相似，0代表不相似。  \n",
    "选择文件train中的string1&2部分作为训练语料，选择文件test计算句子相似度，然后与标注结果比较，输出你认为合适的分类阈值，以及该阈值下的准确率Accuracy，精确率Precision，召回率Recall和F1值（精确到小数点后两位）。  \n",
    "  \n",
    "句向量相似度计算方式：  \n",
    "首先对句子分词，获取每个词的词向量，然后将所有的词向量相加求平均，得到句子向量，最后计算两个句子向量的余弦值(余弦相似度)。  \n",
    "\n",
    "Word2vec部分，使用Gensim的Word2Vec训练模型，自行调整合适的参数。  \n",
    "注意可能会出现word xx not in vocabulary的情况，这是由于训练时默认参数min_count=5，会过滤掉低频词。可以手动设定min_count=1，或者在计算句向量时，遇到低频词则忽略。自行选择其一，并注释。  \n",
    "gensim镜像安装方式  \n",
    "pip install -i https://pypi.tuna.tsinghua.edu.cn/simple gensim  \n",
    "导入方式from gensim.models import word2vec  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一、函数定义&导包\n",
    "1. $getTokenized(sentence)->list$ :输入一个句子，返回其标准化之后的分词列表\n",
    "2. $getSentenceVector(tokenization, trainModel)->np.ndarray$ :输入一个句子的分词列表、训练模型, 返回它的词向量\n",
    "3. $getCosineSimilarity(vector1,vector2)->float$ :输入两个句子的句向量，返回他们的余弦相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk.tokenize as tk\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from scipy.linalg import norm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "#函数1：将句子转换成标准规范化的分词形式\n",
    "def getTokenized(sentence:str)->list:               \n",
    "    pattern=re.compile(\"[^a-zA-Z0-9\\n ]\")              #数字字符的正则匹配\n",
    "    sentence = re.sub(pattern,\"\",sentence).lower()     #将所有非数字字符的符号转化为空，大小写转换\n",
    "    Tokenization = tk.word_tokenize(sentence)          #分词，标记化\n",
    "    Tokenization = [w for w in Tokenization if(w not in stopwords.words('english'))]  #去停用词\n",
    "    return Tokenization\n",
    "\n",
    "\n",
    "#函数2: 输入一个句子的分词列表、训练模型, 返回它的句子向量\n",
    "def getSentenceVector(tokenization:list, trainModel:Word2Vec)->np.ndarray:\n",
    "    vector = np.zeros(100)\n",
    "    for word in tokenization:\n",
    "        vector += trainModel[word]    #trainModel即为该单词的词向量\n",
    "    if len(tokenization) != 0:\n",
    "        vector /= len(tokenization)       #求平均值，得到句子向量\n",
    "    return vector\n",
    "\n",
    "#函数3： 输入两个句子的句向量，返回他们的余弦相似度\n",
    "def getCosineSimilarity(vector1:np.ndarray, vector2:np.ndarray)->float:\n",
    "    # np.dot :向量点乘\n",
    "    # norm:   向量的模\n",
    "    if norm(vector1) == 0 and norm(vector2) == 0:\n",
    "        return 1\n",
    "    elif norm(vector1) == 0 or norm(vector2) == 0:\n",
    "        return 0\n",
    "    return np.dot(vector1, vector2) / (norm(vector1) * norm(vector2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二、数据读取&分词处理\n",
    "- 调用系统的csv读取文本,然后转换成list存储在data中 \n",
    "- 如果open的时候不注明编码方式为utf-8,编译器会抛出一个gbk decode异常\n",
    "- WordsList1[i]表示第i行的string1的分词列表，WordsList2同理\n",
    "- 设置了部分提示信息来表示程序的运行进度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在获取分词:\n",
      "    进度:9%\n",
      "    进度:19%\n",
      "    进度:29%\n",
      "    进度:39%\n",
      "    进度:49%\n",
      "    进度:59%\n",
      "    进度:69%\n",
      "    进度:79%\n",
      "    进度:89%\n",
      "    进度:99%\n",
      "计算分词已完成\n"
     ]
    }
   ],
   "source": [
    "#data[i][0]-data[i][4]分别表示第i项数据的 编号,ID1,ID2,字符串1，字符串2\n",
    "data = open(\"msr_train.csv\",\"r\", encoding=\"utf-8\")   #读取全部数据\n",
    "data = list(csv.reader(data))[1:]                    #去除表头    \n",
    "WordsList1 = []\n",
    "WordsList2 = []\n",
    "\n",
    "print(\"正在获取分词:\")\n",
    "cmp = len(data)//10\n",
    "for i in range (len(data)):\n",
    "    if i == cmp:\n",
    "        print(\"    进度:%d%%\"%(cmp*100//len(data)))\n",
    "        cmp += len(data)//10\n",
    "    WordsList1.append(getTokenized(data[i][3]))\n",
    "    WordsList2.append(getTokenized(data[i][4]))\n",
    "print(\"计算分词已完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三、Word2Vec模型训练\n",
    "- WordsList为传入训练参数\n",
    "- min_count是对词进行过滤，频率小于min-count的单词则会被忽视\n",
    "- sg=1是skip-gram算法，对低频词敏感\n",
    "- window是句子中当前词与目标词之间的最大距离，3表示在目标词前看3-b个词，后面看b个词（b在0-3之间随机）。\n",
    "- size是输出词向量的维数，值太小会导致词映射因为冲突而影响结果，值太大则会耗内存并使算法计算变慢\n",
    "- negative和sample可根据训练结果进行微调，sample表示更高频率的词被随机下采样到所设置的阈值，默认值为1e-3。\n",
    "- hs=1表示层级softmax将会被使用，默认hs=0且negative不为0，则负采样将会被选择使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec模型训练已完成\n"
     ]
    }
   ],
   "source": [
    "Model1 = Word2Vec(WordsList1, min_count= 1, sg = 1, window = 5, size = 100)\n",
    "Model2 = Word2Vec(WordsList2, min_count= 1, sg = 1, window = 5, size = 100)\n",
    "print(\"Word2Vec模型训练已完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 四、计算阈值\n",
    "- 采用全部句子余弦相似度的平均值作为阈值\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-68-fdbd135ebef5>:24: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  vector += trainModel[word]    #trainModel即为该单词的词向量\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "设置阈值为: 0.9279506618798747\n"
     ]
    }
   ],
   "source": [
    "AllSimilarity = []\n",
    "sumValue = 0\n",
    "for i in range(len(data)):\n",
    "    vector1 = getSentenceVector(WordsList1[i],Model1)\n",
    "    vector2 = getSentenceVector(WordsList2[i],Model2)\n",
    "    CosineSimilarity = getCosineSimilarity(vector1,vector2)\n",
    "    AllSimilarity.append(CosineSimilarity)\n",
    "    sumValue += CosineSimilarity\n",
    "averageValue = sumValue / len(AllSimilarity)\n",
    "print(\"设置阈值为:\",averageValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 五、与标注结果对比\n",
    "- StandardResult为msr_train.csv中的标准结果\n",
    "- MyResult为训练的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率Accuracy: 0.68\n",
      "精确率Precision: 0.68\n",
      "召回率Recall: 0.99\n",
      "F1值: 0.81\n"
     ]
    }
   ],
   "source": [
    "StandardResult = [int(e[0]) for e in data]\n",
    "MyResult = []\n",
    "for value in AllSimilarity:\n",
    "    if value >= averageValue:\n",
    "        MyResult.append(1)\n",
    "    else:\n",
    "        MyResult.append(0)\n",
    "\n",
    "print(\"准确率Accuracy: %.2f\"%accuracy_score(StandardResult, MyResult))\n",
    "print(\"精确率Precision: %.2f\"%precision_score(StandardResult, MyResult))\n",
    "print(\"召回率Recall: %.2f\"%recall_score(StandardResult, MyResult))\n",
    "print(\"F1值: %.2f\"%f1_score(StandardResult, MyResult))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
