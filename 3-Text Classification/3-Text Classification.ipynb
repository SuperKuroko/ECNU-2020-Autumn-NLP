{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作业三 朴素贝叶斯情感分类  \n",
    "任务描述：利用nltk语料库中的影评来进行朴素贝叶斯情感分类训练。  \n",
    "影评导入：from nltk.corpus import movie_reviews  \n",
    "文件具体目录在…\\nltk_data\\corpora\\movie_reviews，已做好分类标注，消极与积极影评各1000条。  \n",
    "步骤：经过文本预处理（去噪、分句、分词、去停词、取词干、修剪）和特征选择，生成特征词表，之后利用朴素贝叶斯模型进行训练。（每个步骤最好注释一下）  \n",
    "选择前80%（即前800条消极影评与前800条积极影评）作为训练集，后20%作为测试集。  \n",
    "输出：准确率Accuracy，精确率Precision，召回率Recall和F1值，精确到小数点后两位。其中，F1 = ( 2 * Precision * Recall ) / ( Precision + Recall)。  \n",
    "例如：  \n",
    "Accuracy = 0.98  \n",
    "Precision = 0.67  \n",
    "Recall = 0.32  \n",
    "F1 = 0.43  \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Part1 库的导入 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import nltk.tokenize as tk\n",
    "import nltk.stem as ns\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import movie_reviews\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Part2 文件的读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = str(movie_reviews)   #得到movie_reviews的路径，但还包含其他杂项信息\n",
    "start = path.find(\"'\")      #其中路径被包含在两个引号之间，因此先找到第一个引号的下标\n",
    "end = path.rfind(\"'\")       #再找到下一个引号的下标\n",
    "path = path[start+1:end]    #然后取子串\n",
    "pos_path = path + \"\\\\\\\\pos\" #添加pos为积极影评的路径\n",
    "neg_path = path + \"\\\\\\\\neg\" #添加neg为消极影评的路径\n",
    "\n",
    "pos_comments = []           #存储所有积极评论的文本\n",
    "neg_comments = []           #存储所有消极评论的文本\n",
    " \n",
    "for root,dirs,files in os.walk(pos_path):   #循环目录\n",
    "    for file in files:                      #循环所有文件\n",
    "        fileName = root+\"\\\\\\\\\"+file\n",
    "        f = open(fileName,'r')              #获取文本的绝对路径\n",
    "        pos_comments.append(f.read())       #读取其内容并存储到对应数组中\n",
    "        \n",
    "for root,dirs,files in os.walk(neg_path):  #同上\n",
    "    for file in files:\n",
    "        fileName = root+\"\\\\\\\\\"+file\n",
    "        f = open(fileName,'r')\n",
    "        neg_comments.append(f.read())\n",
    "#print(pos_comments)\n",
    "#print(neg_comments)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Part3 文本处理函数\n",
    "参照作业2的文本处理办法，将800条积极影评/消极影评的文本进行文本处理\n",
    "与作业2不同的是，并不是800条影评的词汇全部加在一起之后进行文本处理\n",
    "而是对每一条进行单独处理，之后再合并存储，目的是方便删去低频词\n",
    "在此基础上，以出现频率为特征进行特征选择\n",
    "并删去低频词，选取排名前20的单词作为函数的返回值，返回类型为Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TextProcess(text):\n",
    "    #Step 1 Tokenization:\n",
    "    Tokenization = tk.word_tokenize(text) #首先逐个拆分\n",
    "    \n",
    "    #Step 2 Normalization:\n",
    "    pattern=re.compile(\"[^a-zA-Z0-9\\n ]\")      #数字字符的正则匹配\n",
    "    Normalization = []\n",
    "    for e in Tokenization:\n",
    "        e = re.sub(pattern,\"\",e).lower()      #将所有非数字字符的符号转化为空，大小写转换\n",
    "        e = tk.word_tokenize(e)               #文本标记化/分词\n",
    "        e = [w for w in e if(w not in stopwords.words('english'))]  #去停用词\n",
    "        Normalization += e                    #t添加到输出\n",
    "    \n",
    "    #Step3 Stemming:                          #词干提取\n",
    "    Stemming = []\n",
    "    for token in Normalization:\n",
    "        pt_stem= nltk.stem.porter.PorterStemmer().stem(token)\n",
    "        Stemming.append(token)\n",
    "\n",
    "    #Step 4 Lemmatization:\n",
    "    Lemmatization = []\n",
    "    lemmatizer = ns.WordNetLemmatizer()\n",
    "    for token in Stemming:\n",
    "        Lemmatization.append(lemmatizer.lemmatize(token))\n",
    "\n",
    "    result = Counter(Lemmatization).most_common(20)   #取最高词频的20个\n",
    "    return result"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Part4 训练集的训练，对于每一个数据\n",
    "获取其文本处理函数的返回值，然后依次添加到字典中\n",
    "其中pos_dict为积极影评的字典，key为单词，value为出现次数\n",
    "neg_dict则是消极影评的字典，key为单词，value为出现次数\n",
    "因为是字典的建立需要大量时间，所以整个训练过程花费时间较多，还请耐心等待"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练可能要花费较长时间，还请耐心等待\n",
      "积极影评已经训练到第0个数据\n",
      "积极影评已经训练到第50个数据\n",
      "积极影评已经训练到第100个数据\n",
      "积极影评已经训练到第150个数据\n",
      "积极影评已经训练到第200个数据\n",
      "积极影评已经训练到第250个数据\n",
      "积极影评已经训练到第300个数据\n",
      "积极影评已经训练到第350个数据\n",
      "积极影评已经训练到第400个数据\n",
      "积极影评已经训练到第450个数据\n",
      "积极影评已经训练到第500个数据\n",
      "积极影评已经训练到第550个数据\n",
      "积极影评已经训练到第600个数据\n",
      "积极影评已经训练到第650个数据\n",
      "积极影评已经训练到第700个数据\n",
      "积极影评已经训练到第750个数据\n",
      "消极影评已经训练到第0个数据\n",
      "消极影评已经训练到第50个数据\n",
      "消极影评已经训练到第100个数据\n",
      "消极影评已经训练到第150个数据\n",
      "消极影评已经训练到第200个数据\n",
      "消极影评已经训练到第250个数据\n",
      "消极影评已经训练到第300个数据\n",
      "消极影评已经训练到第350个数据\n",
      "消极影评已经训练到第400个数据\n",
      "消极影评已经训练到第450个数据\n",
      "消极影评已经训练到第500个数据\n",
      "消极影评已经训练到第550个数据\n",
      "消极影评已经训练到第600个数据\n",
      "消极影评已经训练到第650个数据\n",
      "消极影评已经训练到第700个数据\n",
      "消极影评已经训练到第750个数据\n",
      "训练完成\n"
     ]
    }
   ],
   "source": [
    "pos_dict = {}\n",
    "print(\"训练可能要花费较长时间，还请耐心等待\")\n",
    "for i in range(800):\n",
    "    counter = TextProcess(pos_comments[i])  #\n",
    "    if i%50 == 0:\n",
    "        print(\"积极影评已经训练到第%d个数据\"%i)\n",
    "    for e in counter:\n",
    "        if e[0] in pos_dict:\n",
    "            pos_dict[e[0]] += e[1]\n",
    "        else:\n",
    "            pos_dict[e[0]] = e[1]\n",
    "neg_dict = {}\n",
    "for i in range(800):\n",
    "    if i%50 == 0:\n",
    "        print(\"消极影评已经训练到第%d个数据\"%i)\n",
    "    counter = TextProcess(neg_comments[i])\n",
    "    for e in counter:\n",
    "        if e[0] not in neg_dict:\n",
    "            neg_dict[e[0]] = e[1]\n",
    "        else:\n",
    "            neg_dict[e[0]] += e[1]\n",
    "print(\"训练完成\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Part5 测试集的检验\n",
    "C1为积极影评字典中的单词总数，V1为积极影评字典的规模\n",
    "C2位消极影评字典中的单词总数，V2位消极影评字典的规模\n",
    "检验方法为:\n",
    "对于测试集的每个影评，首先通过文本处理函数获取其最高的20个单词以及其出现次数\n",
    "然后对于每个单词，计算其相应的概率\n",
    "P1表示其为积极影评的概率\n",
    "P2表示其为消极影评的概率\n",
    "在计算的过程中不断更新\n",
    "最后通过比较P1和P2的大小来增加TP，FN，FP，TN的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已经测试完第0个积极影评\n",
      "已经测试完第50个积极影评\n",
      "已经测试完第100个积极影评\n",
      "已经测试完第150个积极影评\n",
      "已经测试完第0个消极影评\n",
      "已经测试完第50个消极影评\n",
      "已经测试完第100个消极影评\n",
      "已经测试完第150个消极影评\n",
      "测试完成\n"
     ]
    }
   ],
   "source": [
    "C1 = 0     \n",
    "C2 = 0\n",
    "V1 = len(pos_dict)\n",
    "V2 = len(neg_dict)\n",
    "for e in pos_dict:\n",
    "    C1 += pos_dict[e]\n",
    "for e in neg_dict:\n",
    "    C2 += neg_dict[e]\n",
    "#首先计算好C1,C2,V1,V2的值\n",
    "\n",
    "#积极影评的测试\n",
    "TP = 0    \n",
    "FN = 0\n",
    "for i in range(800,1000):\n",
    "    if(i%50 == 0):\n",
    "        print(\"已经测试完第%d个积极影评\"%(i-800))\n",
    "    counter = TextProcess(pos_comments[i]) #得到最高频的20个单词的键值对\n",
    "    P1 = 1\n",
    "    P2 = 1\n",
    "    for e in counter:                 #对于每个键值对\n",
    "        word = e[0]                   #获取单词\n",
    "        if word not in pos_dict:     #若不在字典中\n",
    "            val = 1                   #val为1\n",
    "        else:\n",
    "            val = pos_dict[word]+1   #否则val = 1+该单词的出现次数\n",
    "        P1 *= pow(val/(C1+V1),e[1])  #更新P1的值\n",
    "        \n",
    "        if word not in neg_dict:    #同上\n",
    "            val = 1\n",
    "        else:\n",
    "            val = neg_dict[word]+1\n",
    "        P2 *= pow(val/(C2+V2),e[1])\n",
    "        \n",
    "    #根据P1和P2的大小比较来判断测试结果\n",
    "    if P1 >= P2:\n",
    "        TP += 1\n",
    "    else:\n",
    "        FN += 1\n",
    "\n",
    "        \n",
    "#消极影评的测试，其步骤同上\n",
    "FP = 0\n",
    "TN = 0\n",
    "for i in range(800,1000):\n",
    "    counter = TextProcess(neg_comments[i])\n",
    "    if(i%50 == 0):\n",
    "        print(\"已经测试完第%d个消极影评\"%(i-800))\n",
    "    P1 = 1\n",
    "    P2 = 1\n",
    "    for e in counter:\n",
    "        word = e[0]\n",
    "        if word not in pos_dict:\n",
    "            val = 1\n",
    "        else:\n",
    "            val = pos_dict[word]+1\n",
    "        P1 *= pow(val/(C1+V1),e[1])\n",
    "        \n",
    "        if word not in neg_dict:\n",
    "            val = 1\n",
    "        else:\n",
    "            val = neg_dict[word]+1\n",
    "        P2 *= pow(val/(C2+V2),e[1])\n",
    "        \n",
    "    if P1 <= P2:\n",
    "        TN += 1\n",
    "    else:\n",
    "        FP += 1\n",
    "print(\"测试完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "Part6 结果输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP =  139 FP =  48\n",
      "FN =  61 TN =  152\n",
      "Accuracy =  0.7275\n",
      "Precision =  0.7433155080213903\n",
      "Recall =  0.695\n",
      "F1 =  0.7183462532299741\n"
     ]
    }
   ],
   "source": [
    "print(\"TP = \",TP,\"FP = \",FP)\n",
    "print(\"FN = \",FN,\"TN = \",TN)\n",
    "Accuracy = (TP+TN) / (TP+TN+FN+FP)\n",
    "Precision = TP / (TP+FP)\n",
    "Recall = TP / (TP+FN)\n",
    "F1 = (2*Precision*Recall) / (Precision+Recall)\n",
    "print(\"Accuracy = \",Accuracy)\n",
    "print(\"Precision = \",Precision)\n",
    "print(\"Recall = \",Recall)\n",
    "print(\"F1 = \",F1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
